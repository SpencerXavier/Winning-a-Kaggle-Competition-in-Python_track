0. Kaggle prerequiste : 
```
Process: problem -> data -> model -> submisssion -> leaderboard [查kaggle 如何寫成CV]
composition: train data [we need to label them] + test data 
```
## define problem type and explore the data

1-1. Explore train data 
```
# Import pandas
import pandas as pd

# Read train data
train = pd.read_csv('train.csv')

# Look at the shape of the data
print('Train shape:', train.shape)

# Look at the head() of the data
print(train.head())
```

1-2. Explore test data
```
import pandas as pd

# Read the test data
test = pd.read_csv('test.csv')

# Print train and test columns
print('Train columns:', train.columns.tolist()) 
print('Test columns:', test.columns.tolist())

ps. .columns is an attribute, show columns names of dataframe
ps. .tollist() is a method
ps. Remember, that the test dataset generally contains one column less than the train one. 
    This column, is presented in the sample submission file. [output]

```
1-3. Determine a problem type
```
題目問 asked to predict 3 months of the future sales
train.sales.hist(bins=30,alpha=0.5)....bins代表这个参数指定bin(箱子)的个数,也就是总共有几条条状图, train.sales為x軸 , y軸為數量
plt.show()
->發現The sales variable is continuous  -> regression problem
ps.迴歸 (regression) 方法是一個分析變數和變數之間關係的工具，主要在探討自變數(x)與依變數(y)之間的線性關係，透過迴歸模型的建立，可以推論和預測研究者感興趣的變數(y)。

```

## put data into a model
2-1. Train a simple model
```
import pandas as pd
from sklearn.ensemble import RandomForestRegressor.  ....常. 用的regression model

# Read the train data
train = pd.read_csv('train.csv')

# Create a Random Forest object
rf = RandomForestRegressor()

# Train a model
rf.fit(X=train[['store','item']], y=train['sales'])

ps. RandomForestRegressor 為一種機器學習訓練模型的方法
ps. 先建立模型物件，再fit data, x為自變量, y為應變量
```

2-2. Test model ,write into submission file
```
# Read test and sample submission data
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

# Show the head() of the sample_submission
print(sample_submission.head())

# Read test and sample submission data
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

# Show the head() of the sample_submission
print(sample_submission.head())

# Get predictions for the test set
test['sales'] = rf.predict(test[['store', 'item']])...得到y的應變量

# Write test predictions using the sample_submission format - 覆寫提交檔
test[['id','sales']].to_csv('kaggle_submission.csv', index=False)

ps.column_name 一個[]
ps. [[]] 2個代表dataframe

```

## Public vs Private leaderboard - optimize the metric given
```
What is overgiting?
-> After fit the model , there willl be some error rate on train and test data
-> When increase model complexity , the train data error generally goes down , it happens beacuse model learns the data so well, than it performs great on it with very little error
-> However, test error goes up
->when test error goes up , train error goes down , the moment is the starting point of overfitting.
-> Model 4 has considerably lower train MSE compared to other models. However, validation MSE started growing again.

The goal of kaggle competition?
-> optimize  competetion metric -  optimize problem type 的解法
-> 使Mean squer error(MSE)最小

```
ex-0
```
The goal of this exercise is to determine whether any of the models trained is overfitting. To measure the quality of the models you will use Mean Squared Error (MSE). It's available in sklearn.metrics as mean_squared_error() function that takes two arguments: true values and predicted values.

```

ex-1. Train XGBoost models
```
-> Every Machine Learning method could potentially overfit. Let's take XGboost for example.
import xgboost as xgb

# Create DMatrix on train data
dtrain = xgb.DMatrix(data=train[['store', 'item']],
                     label=train['sales'])
# Define xgboost parameters
params = {'objective': 'reg:linear',
          'max_depth': 2,
          'silent': 1}

ps.max_depth - maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
ps.其他參數之後會教

# Train xgboost model
xg_depth_2 = xgb.train(params=params, dtrain=dtrain)

```

ex-2. Explore overfitting XGBoost 
```
a.explore train data and test data + determine a problem type- already set up
dtrain = xgb.DMatrix(data=train[['store', 'item']])
dtest = xgb.DMatrix(data=test[['store', 'item']])

b.train a simple model and test model [already train]

c.test the overfitting
from sklearn.metrics import mean_squared_error

# For each of 3 trained models
for model in [xg_depth_2, xg_depth_8, xg_depth_15]:
    # Make predictions
    train_pred = model.predict(dtrain)     
    test_pred = model.predict(dtest)          
    
    # Calculate metrics
    mse_train = mean_squared_error(train['sales'], train_pred)                  
    mse_test = mean_squared_error(test['sales'], test_pred)
    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))

ps. 當我們發現 MSE train 在下降，但 MSE test 在上升，就是overfittting的時候
```

## EDA - 補足data manipulation + 補足 EDA + 補足 data visualization 
```
understand the problem -> EDA -> locak validation -> modeling 
```
1. Understand the problem
```
datatype: tabular data, time series, images, text
problem type: classification, regression, ranking
evaluation metric: ROC,AUC, MSE....

ps.predict-> regression problem
````

1-2.Define a competition metric
```
1.Mean Squared Error (MSE) for the regression problem
import numpy as np
# Import MSE from sklearn
from sklearn.metrics import mean_squared_error

# Define your own MSE function
def own_mse(y_true, y_pred):
  	# Raise differences to the power of 2
    diff = y_true - y_pred
    squares = np.power(diff, 2)
    # Find mean over all observations
    err = np.mean(squares)
    return err

print('Sklearn MSE: {:.5f}. '.format(mean_squared_error(y_regression_true, y_regression_pred)))
print('Your MSE: {:.5f}. '.format(own_mse(y_regression_true, y_regression_pred)))

2.Logarithmic Loss (LogLoss) for the binary classification problem:
import numpy as np

# Import log_loss from sklearn
from sklearn.metrics import log_loss

# Define your own LogLoss function
def own_logloss(y_true, prob_pred):
  	# Find loss for each observation
    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)
    # Find mean over all observations
    err = np.mean(terms) 
    return -err

print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))
print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))

```
2. EDA[classification for example] 
```
a.Goal of EDA
- size of the data
- properties of the target variable
- properties of features
- gernerate ideas for feature engineering

b.EDA size and statistics - 基本資料
# Shapes of train and test data
print('Train shape:', train.shape)
print('Test shape:', test.shape)

# Train head()
print(train.head())

# Describe the target variable
print(train.fare_amount.describe())

# Train distribution of passengers within rides
print(train.passenger_count.value_counts())

c. EDA plots I - 猜測距離和車資有關
# Calculate the ride distance
train['distance_km'] = haversine_distance(train)

# Draw a scatterplot
plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)
plt.xlabel('Fare amount')
plt.ylabel('Distance, km')
plt.title('Fare amount based on the distance')

# Limit on the distance
plt.ylim(0,50)
plt.show()


d. EDA plots II - 猜測時間和車資有關

# Create hour feature
train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)
train['hour'] = train.pickup_datetime.dt.hour


# Find median fare_amount for each hour
hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()


# Plot the line plot
plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')
plt.xlabel('Hour of the day')
plt.ylabel('Median fare amount')
plt.title('Fare amount based on day time')
plt.xticks(range(24))
plt.show()

ps.skewed distribution 偏態分佈

```

3. basic Local validation - different methods
```
ps. holdout set[test data] is used to evaluate the model
ps. K-fold cross validation - we split the train data into K non-overlapping parts called 'folds' and train model k times with k times different holdout set
ps. n_splits is number of folds, shuffle is the data is sorted before spliting[最好都設TRue], random_state sets a seed to reproduce the same folds in future run
ps. split method returns a list of training and testing observations for each split
ps. train_index and test_index can be used to select cross-validation split data[train and holdout set]


# Import KFold
from sklearn.model_selection import KFold

# Create a KFold object
kf = KFold(n_splits= 3 , shuffle=True, random_state=123)

# Loop through each split
fold = 0
for fold, (train_index, test_index) in enumerate(kf.split(train)):
    # Obtain training and testing folds
    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
    print('Fold: {}'.format(fold))
    print('CV train shape: {}'.format(cv_train.shape))
    print('Medium interest listings in CV train: {}\n'.format(sum(cv_train.interest_level == 'medium')))
    # fold += 1

ps. fold 為 list 前面的index , in enumerate 會顯示 list 前面的index
ps. kf.split 會 return train 和 test 的 slice index (用tuple 包住ndarray的形式)
ps. another approach is called stratified K-fold //// is same as K-fold but creates stratified folds by a target variable.
ps. These folds are made by preserving the percentage of samples for each class of this variable.
ps. useful when we have classification with class imbalance in the target variable or our data size is small

# Import StratifiedKFold
from sklearn.model_selection import StratifiedKFold

# Create a StratifiedKFold object
str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)

# Loop through each split
fold = 0
for fold , (train_index, test_index) in enumerate(str_kf.split(train, train['interest_level'])):
    # Obtain training and testing folds
    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
    print('Fold: {}'.format(fold))
    print('CV train shape: {}'.format(cv_train.shape))
    print('Medium interest listings in CV train: {}\n'.format(sum(cv_train.interest_level == 'medium')))

```
4. Validation usage
```
different types of leakage 
a. leak in features -  using data that will not be avaliable in the 
b. leak in validation strategy differs from the real-world problem -> for example, time series的資料如果用kfold 會抓取未來的資料，但real-world沒有未來的資料
c.
d.







leakage causes a model to seem accurate until we start making predictions in a real-world environment. -> useless model
```


















































## Feature engineering






























