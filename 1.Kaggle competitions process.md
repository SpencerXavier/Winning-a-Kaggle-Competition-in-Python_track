0. Kaggle prerequiste : 
```
Process: problem -> data -> model -> submisssion -> leaderboard [查kaggle 如何寫成CV]
composition: train data [we need to label them] + test data 
```
## define problem type and explore the data - 補足 python, intermediate python, data manipulation

1-1. Explore train data 
```
# Import pandas
import pandas as pd

# Read train data
train = pd.read_csv('train.csv')

# Look at the shape of the data
print('Train shape:', train.shape)

# Look at the head() of the data
print(train.head())
```

1-2. Explore test data
```
import pandas as pd

# Read the test data
test = pd.read_csv('test.csv')

# Print train and test columns
print('Train columns:', train.columns.tolist()) 
print('Test columns:', test.columns.tolist())

ps. .columns is an attribute, show columns names of dataframe
ps. .tollist() is a method
ps. Remember, that the test dataset generally contains one column less than the train one. 
    This column, is presented in the sample submission file. [output]

```
1-3. Determine a problem type
```
題目問 asked to predict 3 months of the future sales
train.sales.hist(bins=30,alpha=0.5)....bins代表这个参数指定bin(箱子)的个数,也就是总共有几条条状图, train.sales為x軸 , y軸為數量
plt.show()
->發現The sales variable is continuous  -> regression problem
ps.迴歸 (regression) 方法是一個分析變數和變數之間關係的工具，主要在探討自變數(x)與依變數(y)之間的線性關係，透過迴歸模型的建立，可以推論和預測研究者感興趣的變數(y)。

```

## put data into a model
2-1. Train a simple model
```
import pandas as pd
from sklearn.ensemble import RandomForestRegressor.  ....常. 用的regression model

# Read the train data
train = pd.read_csv('train.csv')

# Create a Random Forest object
rf = RandomForestRegressor()

# Train a model
rf.fit(X=train[['store','item']], y=train['sales'])

ps. RandomForestRegressor 為一種機器學習訓練模型的方法
ps. 先建立模型物件，再fit data, x為自變量, y為應變量
```

2-2. Test model ,write into submission file
```
# Read test and sample submission data
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

# Show the head() of the sample_submission
print(sample_submission.head())

# Read test and sample submission data
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

# Show the head() of the sample_submission
print(sample_submission.head())

# Get predictions for the test set
test['sales'] = rf.predict(test[['store', 'item']])...得到y的應變量

# Write test predictions using the sample_submission format - 覆寫提交檔
test[['id','sales']].to_csv('kaggle_submission.csv', index=False)

ps.column_name 一個[]
ps. [[]] 2個代表dataframe

```

## Public vs Private leaderboard - optimize the metric given
```
What is overgiting?
-> After fit the model , there willl be some error rate on train and test data
-> When increase model complexity , the train data error generally goes down , it happens beacuse model learns the data so well, than it performs great on it with very little error
-> However, test error goes up
->when test error goes up , train error goes down , the moment is the starting point of overfitting.
-> Model 4 has considerably lower train MSE compared to other models. However, validation MSE started growing again.

The goal of kaggle competition?
-> optimize  competetion metric -  optimize problem type 的解法
-> 使Mean squer error(MSE)最小

```
ex-0
```
The goal of this exercise is to determine whether any of the models trained is overfitting. To measure the quality of the models you will use Mean Squared Error (MSE). It's available in sklearn.metrics as mean_squared_error() function that takes two arguments: true values and predicted values.

```

ex-1. Train XGBoost models
```
-> Every Machine Learning method could potentially overfit. Let's take XGboost for example.
import xgboost as xgb

# Create DMatrix on train data
dtrain = xgb.DMatrix(data=train[['store', 'item']],
                     label=train['sales'])
# Define xgboost parameters
params = {'objective': 'reg:linear',
          'max_depth': 2,
          'silent': 1}

ps.max_depth - maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
ps.其他參數之後會教

# Train xgboost model
xg_depth_2 = xgb.train(params=params, dtrain=dtrain)

```

ex-2. Explore overfitting XGBoost 
```
a.explore train data and test data + determine a problem type- already set up
dtrain = xgb.DMatrix(data=train[['store', 'item']])
dtest = xgb.DMatrix(data=test[['store', 'item']])

b.train a simple model and test model [already train]

c.test the overfitting
from sklearn.metrics import mean_squared_error

# For each of 3 trained models
for model in [xg_depth_2, xg_depth_8, xg_depth_15]:
    # Make predictions
    train_pred = model.predict(dtrain)     
    test_pred = model.predict(dtest)          
    
    # Calculate metrics
    mse_train = mean_squared_error(train['sales'], train_pred)                  
    mse_test = mean_squared_error(test['sales'], test_pred)
    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))

ps. 當我們發現 MSE train 在下降，但 MSE test 在上升，就是overfittting的時候
```

## EDA -  補足 EDA + 補足 data visualization 
```
understand the problem -> EDA -> locak validation -> modeling 
```
1. Understand the problem
```
datatype: tabular data, time series, images, text
problem type: classification, regression, ranking
evaluation metric: ROC,AUC, MSE....

ps.predict-> regression problem
````

1-2.Define a competition metric
```
1.Mean Squared Error (MSE) for the regression problem
import numpy as np
# Import MSE from sklearn
from sklearn.metrics import mean_squared_error

# Define your own MSE function
def own_mse(y_true, y_pred):
  	# Raise differences to the power of 2
    diff = y_true - y_pred
    squares = np.power(diff, 2)
    # Find mean over all observations
    err = np.mean(squares)
    return err

print('Sklearn MSE: {:.5f}. '.format(mean_squared_error(y_regression_true, y_regression_pred)))
print('Your MSE: {:.5f}. '.format(own_mse(y_regression_true, y_regression_pred)))

2.Logarithmic Loss (LogLoss) for the binary classification problem:
import numpy as np

# Import log_loss from sklearn
from sklearn.metrics import log_loss

# Define your own LogLoss function
def own_logloss(y_true, prob_pred):
  	# Find loss for each observation
    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)
    # Find mean over all observations
    err = np.mean(terms) 
    return -err

print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))
print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))

```
2. EDA[classification for example] 
```
a.Goal of EDA
- size of the data
- properties of the target variable
- properties of features
- gernerate ideas for feature engineering

b.EDA size and statistics - 基本資料
# Shapes of train and test data
print('Train shape:', train.shape)
print('Test shape:', test.shape)

# Train head()
print(train.head())

# Describe the target variable
print(train.fare_amount.describe())

# Train distribution of passengers within rides
print(train.passenger_count.value_counts())

c. EDA plots I - 猜測距離和車資有關
# Calculate the ride distance
train['distance_km'] = haversine_distance(train)

# Draw a scatterplot
plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)
plt.xlabel('Fare amount')
plt.ylabel('Distance, km')
plt.title('Fare amount based on the distance')

# Limit on the distance
plt.ylim(0,50)
plt.show()


d. EDA plots II - 猜測時間和車資有關

# Create hour feature
train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)
train['hour'] = train.pickup_datetime.dt.hour


# Find median fare_amount for each hour
hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()


# Plot the line plot
plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')
plt.xlabel('Hour of the day')
plt.ylabel('Median fare amount')
plt.title('Fare amount based on day time')
plt.xticks(range(24))
plt.show()

ps.skewed distribution 偏態分佈

```

3. basic Local validation - different methods 
```
ps. holdout set[test data] is used to evaluate the model
ps. K-fold cross validation - we split the train data into K non-overlapping parts called 'folds' and train model k times with k times different holdout set 拿一部分的資料training，另一部分的資料測試 
ps. n_splits is number of folds, shuffle is the data is sorted before spliting[最好都設TRue], random_state sets a seed to reproduce the same folds in future run
ps. split method returns a list of training and testing observations for each split
ps. train_index and test_index can be used to select cross-validation split data[train and holdout set]


# Import KFold
from sklearn.model_selection import KFold

# Create a KFold object
kf = KFold(n_splits= 3 , shuffle=True, random_state=123)

# Loop through each split
fold = 0
for fold, (train_index, test_index) in enumerate(kf.split(train)):
    # Obtain training and testing folds
    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
    print('Fold: {}'.format(fold))
    print('CV train shape: {}'.format(cv_train.shape))
    print('Medium interest listings in CV train: {}\n'.format(sum(cv_train.interest_level == 'medium')))
    # fold += 1

ps. fold 為 list 前面的index , in enumerate 會顯示 list 前面的index
ps. kf.split 會 return train 和 test 的 slice index (用tuple 包住ndarray的形式)
ps. another approach is called stratified K-fold //// is same as K-fold but creates stratified folds by a target variable.
ps. These folds are made by preserving the percentage of samples for each class of this variable.
ps. useful when we have classification with class imbalance in the target variable or our data size is small

# Import StratifiedKFold
from sklearn.model_selection import StratifiedKFold

# Create a StratifiedKFold object
str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)

# Loop through each split
fold = 0
for fold , (train_index, test_index) in enumerate(str_kf.split(train, train['interest_level'])):
    # Obtain training and testing folds
    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
    print('Fold: {}'.format(fold))
    print('CV train shape: {}'.format(cv_train.shape))
    print('Medium interest listings in CV train: {}\n'.format(sum(cv_train.interest_level == 'medium')))

```
4. Validation usage
```
different types of leakage : leakage causes a model to seem accurate until we start making predictions in a real-world environment. -> useless model
a. leak in features -  using data that will not be avaliable in the future
b. leak in validation strategy differs from the real-world problem -> for example, time series的資料如果用kfold 會抓取未來的資料，但real-world沒有未來的資料
-> 使用 Time K-fold validation

# Create TimeSeriesSplit object
time_kfold = TimeSeriesSplit(n_splits=3)

# Sort train data by date
train = train.sort_values('date')

# Iterate through each split
fold = 0
for fold, (train_index, test_index) in enumerate (time_kfold.split(train)):
    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
    print('Fold :', fold)
    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))
    print('Test date range: from {} to {}\n'.format(cv_test.date.min(), cv_test.date.max()))


c.validation pipeline
a.list for the results 
fold_metrics = []

b.train model and validate
for train_index, test_index in CV_STRATEGY.split(train):
    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
    #train a model
    model.fit(cv_train)
    # make prediction
    predictions = model.predict(cv_test)
    # calculate the metric
    metric = evaluate(cv_test,predictions)
    fold_metrics.append(metric)
    
c.model comparison - get overall validation score first
mean_score = np.mean(fold_metrics)
overall_score_minimizing = np.mean(fold_metrics) + np.std(fold_metrics)
overall_score_maximizing = np.mean(fold_metrics) - np.std(fold_metrics)


from sklearn.model_selection import TimeSeriesSplit
import numpy as np

# Sort train data by date
train = train.sort_values('date')

# Initialize 3-fold time cross-validation
kf = TimeSeriesSplit(n_splits=3)

# Get MSE scores for each cross-validation split
mse_scores = get_fold_mse(train, kf)

print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))
print('MSE by fold: {}'.format(mse_scores))
print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))


ps. For simplicity, you're given get_fold_mse() function that for each cross-validation split fits a Random Forest model and returns a list of MSE scores by fold. get_fold_mse() accepts two arguments: train and TimeSeriesSplit object
ps.To calculate the overall score, find the sum of MSE mean and standard deviation.

```

## Feature engineering: create new Features
```
understand the problem -> EDA -> locak validation -> modeling =  preprocess Data -> create New Features -> improve models -> apply tricks
```
1. Feature engineering -1
```
ps.the simplest engineered feature - arithmetical features/ Datetime features
a. Feature types
- Numerical features
- categorical features
- binary features
- Date features
- coordinates features
- text features
- images features


b.create features to lower MSE
# Look at the initial RMSE
print('RMSE before feature engineering:', get_kfold_rmse(train))

# Find the total area of the house
train['TotalArea'] = train['TotalBsmtSF'] + train['FirstFlrSF'] + train['SecondFlrSF']
print('RMSE with total area:', get_kfold_rmse(train))

# Find the area of the garden
train['GardenArea'] = train['LotArea'] - train['FirstFlrSF']
print('RMSE with garden area:', get_kfold_rmse(train))

# Find total number of bathrooms
train['TotalBath'] =  train['FullBath'] + train['HalfBath']
print('RMSE with number of bathrooms:', get_kfold_rmse(train))

ps. get_kfold_rmse(train) returns a list of MSE scores by fold

```
1-2. Feature engineering -2
```
# Concatenate train and test together
taxi = pd.concat([train, test])

# Convert pickup date to datetime object
taxi['pickup_datetime'] =pd.to_datetime(taxi['pickup_datetime'])

# Create a day of week feature
taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek

# Create an hour feature
taxi['hour'] = taxi['pickup_datetime'].dt.hour

# Split back into train and test
new_train = taxi[taxi['id'].isin(train['id'])]
new_test = taxi[taxi['id'].isin(test['id'])]

```
2. categorical features
```
ps.label encoding - model usually do not handle string -> transfer string to number
# Concatenate train and test together
houses = pd.concat([train, test])

# Label encoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()g

# Create new features
houses['RoofStyle_enc'] = le.fit_transform(houses['RoofStyle'])
houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])

# Look at new features
print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())

ps.However, the approach is harmful for linear model, but it works for tree-based models
-> so we use one-hot encoding instead

ps.df['col_name'].value_counts() 可以count 單個column不同的variable 數目
# One hot encoding -- we create columns instead of single one ,set 1 for the corresponding category and 0 for other categories
one = pd.get_dummies(df['cat'],prefix='ohe_cat') ... prefix parameter will assign column names
df.drop('cat', axis=1,inplace= True) ...then, we drop initial feature column
df = pd.concat([df,ohe],axis=1) ....concatenate the original features

# binary features
le = LabelEncoder()
binary_feature['binary'] = le.fit_transform(binary_feature['binary_feet'])
Yes,no-> 1,0
```

3. Target encoding - 用機率去代表variable被選到的機率
```
a.label encoder provides distinct number for each category...缺點：the approach is harmful for linear model, but it works for tree-based models
One-hot encoder creates new feature for each category value 缺點：要create 太多column
target encoding: it creates only a single column, but it also introduces the correlation between the categories and the target variable, such as
mean target encoding

b.Mean target encoding
step 1: Calculate the mean on the train, apply to the test
step 2: Split train into K folds. Calculate on (K-1) folds [the out-of-fold]mean for each fold, apply to this particular fold -> prevent overfitting of the train set 
step 3: Add mean target encoded feature to the model

c.practical guides
smoothing ......改變step 1算train mean apply to the test的方式 -> 會更準確
normally: we use simple mean
However, if we had rare categories with only one or two values, they would get a strict O or 1 mean encoding.->cause overfitting
smooth_mean_enci = target_sumi + a*global_mean / (ni+ a) ....  阿發is hyperparameter通常是5-10
global mean 代表 整個 target_value mean of train data 
d.New categories : fill new categories in the test data with global_mean

ex: step1: Calculate the  mean on the train, apply to the test mean feature
def test_mean_target_encoding(train, test, target, categorical, alpha=5):
    # Calculate global mean on the train data
    global_mean = train[target].mean()
    
    # Group by the categorical feature and calculate its properties
    train_groups = train.groupby(categorical) 
    category_sum = train_groups[target].sum()
    category_size = train_groups.size()
    
    # Calculate smoothed mean target statistics
    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)
    
    # Apply statistics to the test data and fill new categories
    test_feature = test[categorical].map(train_statistics).fillna(global_mean)
    return test_feature.values
    ps. test_feature is for a new column 顯示機率
    
ex: step2: Split train into K folds. Calculate on the out-of-fold mean for each fold, apply to this particular fold -> prevent overfitting of the train set
def train_mean_target_encoding(train, target, categorical, alpha=5):
    # Create 5-fold cross-validation object
    kf = KFold(n_splits=5, random_state=123, shuffle=True)
    train_feature = pd.Series(index=train.index)

    # For each folds split
    for train_index, test_index in kf.split(train):
        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
        
        # Calculate out-of-fold statistics and apply to cv_test [算出各個test_feature]
        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)
        
        # Save new feature for this particular fold
        train_feature.iloc[test_index] = cv_test_feature       
    return train_feature.values
    
exstep 3: Add mean target encoded feature to the model
def mean_target_encoding(train, test, target, categorical, alpha=5):
  
    # Get the train feature
    train_feature = train_mean_target_encoding(train, target, categorical, alpha)
  
    # Get the test feature
    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)
    
    # Return new features to add to the model
    return train_feature, test_feature
```
4. 
```

```






















