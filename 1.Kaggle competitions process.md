0. Kaggle prerequiste : 
```
Process: problem -> data -> model -> submisssion -> leaderboard [查kaggle 如何寫成CV]
composition: train data [we need to label them] + test data 
```
1-1. Explore train data 
```
# Import pandas
import pandas as pd

# Read train data
train = pd.read_csv('train.csv')

# Look at the shape of the data
print('Train shape:', train.shape)

# Look at the head() of the data
print(train.head())
```

1-2. Explore test data
```
import pandas as pd

# Read the test data
test = pd.read_csv('test.csv')

# Print train and test columns
print('Train columns:', train.columns.tolist()) 
print('Test columns:', test.columns.tolist())

ps. .columns is an attribute, show columns names of dataframe
ps. .tollist() is a method
ps. Remember, that the test dataset generally contains one column less than the train one. 
    This column, is presented in the sample submission file. [output]

```
1-3. Determine a problem type
```
題目問 asked to predict 3 months of the future sales
train.sales.hist(bins=30,alpha=0.5)
plt.show()
->發現The sales variable is continuous  -> regression problem
ps.迴歸 (regression) 方法是一個分析變數和變數之間關係的工具，主要在探討自變數(x)與依變數(y)之間的線性關係，透過迴歸模型的建立，可以推論和預測研究者感興趣的變數(y)。
```

2-1. Train a simple model
```
import pandas as pd
from sklearn.ensemble import RandomForestRegressor

# Read the train data
train = pd.read_csv('train.csv')

# Create a Random Forest object
rf = RandomForestRegressor()

# Train a model
rf.fit(X=train[['store','item']], y=train['sales'])

ps. RandomForestRegressor 為一種機器學習訓練模型的方法
ps. 先建立模型物件，再fit data, x為自變量, y為應變量
```

2-2. Test model ,write into submission file
```
# Read test and sample submission data
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

# Show the head() of the sample_submission
print(sample_submission.head())

# Read test and sample submission data
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

# Show the head() of the sample_submission
print(sample_submission.head())

# Get predictions for the test set
test['sales'] = rf.predict(test[['store', 'item']])...得到y的應變量

# Write test predictions using the sample_submission format - 覆寫提交檔
test[['id','sales']].to_csv('kaggle_submission.csv', index=False)

ps.column_name 一個[]
ps. [[]] 2個代表dataframe

```

ps.Public vs Private leaderboard
```
What is overgiting? 
-> After fit the model , there willl be some error rate on train and test data
-> When increase model complexity , the train data error generally goes down , it happens beacuse model learns the data so well, than it performs great on it with very little error
-> However, test error goes up
->when test error goes up , train error goes down , the moment is the starting point of overfitting.
-> Model 4 has considerably lower train MSE compared to other models. However, validation MSE started growing again.

The goal of kaggle competition?
-> optimize  competetion metric -  optimize problem type 的解法
-> 使Mean squer error(MSE)最小

```
ex-0
```
The goal of this exercise is to determine whether any of the models trained is overfitting. To measure the quality of the models you will use Mean Squared Error (MSE). It's available in sklearn.metrics as mean_squared_error() function that takes two arguments: true values and predicted values.

```

ex-1. Train XGBoost models
```
-> Every Machine Learning method could potentially overfit. Let's take XGboost for example.
import xgboost as xgb

# Create DMatrix on train data
dtrain = xgb.DMatrix(data=train[['store', 'item']],
                     label=train['sales'])
# Define xgboost parameters
params = {'objective': 'reg:linear',
          'max_depth': 2,
          'silent': 1}

ps.max_depth - maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
ps.其他參數之後會教

# Train xgboost model
xg_depth_2 = xgb.train(params=params, dtrain=dtrain)

```

ex-2. Explore overfitting XGBoost 
```
a.explore train data and test data + determine a problem type- already set up
dtrain = xgb.DMatrix(data=train[['store', 'item']])
dtest = xgb.DMatrix(data=test[['store', 'item']])

b.train a simple model and test model [already train]

c.test the overfitting
from sklearn.metrics import mean_squared_error

# For each of 3 trained models
for model in [xg_depth_2, xg_depth_8, xg_depth_15]:
    # Make predictions
    train_pred = model.predict(dtrain)     
    test_pred = model.predict(dtest)          
    
    # Calculate metrics
    mse_train = mean_squared_error(train['sales'], train_pred)                  
    mse_test = mean_squared_error(test['sales'], test_pred)
    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))

ps. 當我們發現 MSE train 在下降，但 MSE test 在上升，就是overfittting的時候
```


